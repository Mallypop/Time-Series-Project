\documentclass[11pt, a4paper]{article} %or article has only section and below, book and report also have chapter: http://texblog.org/2007/07/09/documentclassbook-report-article-or-letter/
\usepackage{float}
\usepackage[utf8]{inputenc} % use utf8 encoding of symbols such as umlaute for maximal compatibility across platforms
\usepackage{caption} % provides commands for handling caption sizes etc.
%\usepackage[a4paper, left=25mm, right=20mm, top=25mm, bottom=20mm]{geometry} % to easily change margin widths: https://www.sharelatex.com/learn/Page_size_and_margins
\usepackage[bottom]{footmisc} % I love footnotes! And they should be down at the bottom of the page!
\usepackage{graphicx} % when using figures and alike
\usepackage[hidelinks]{hyperref} % for hyperreferences (links within the document: references, figures, tables, citations)
\usepackage{euler} % a math font, only for equations and alike; call BEFORE changing the main font; alternatives: mathptmx, fourier,
%\usepackage{gentium} % for a different font; you can also try: cantarell, charter, libertine, gentium, bera, ... http://tex.stackexchange.com/questions/59403/what-font-packages-are-installed-in-tex-live
%------------------------------------------------------------------------------------------------------
%------- text size settings --------------
\setlength{\textwidth}{16cm}%
\setlength{\textheight}{25cm} %23
%(these values were used to fill the page more fully and thus reduce the number of pages!)
\setlength{\topmargin}{-1.5cm} %0
\setlength{\footskip}{1cm} %
%\setlength{\hoffset}{0cm} %
\setlength{\oddsidemargin}{1cm}%
\setlength{\evensidemargin}{-.5cm}%
\setlength{\parskip}{0cm} % Abstand zwischen Absätzen
% ----------------------------------------------------------------
\renewcommand{\textfraction}{0.1} % allows more space to graphics in float
\renewcommand{\topfraction}{0.85}
%\renewcommand{\bottomfraction}{0.65}
\renewcommand{\floatpagefraction}{0.70}
\frenchspacing %http://texwelt.de/wissen/fragen/1154/was-ist-french-spacing-was-macht-frenchspacing
%------------------------------------------------------------------------------------------------------
%------------------------------------------------------------------------------------------------------
\usepackage{Sweave}
\begin{document}
\input{sweaveclean-concordance}
\title{Time Series Analysis - A Tutorial}
\author{Rosskopf,E.; Cordes, M.; Lumiko, J.}
% for more control, multiple affiliations, line breaks and alike, use the authblk package!!
\date{\today} % !!use package isodate for more control of date formatting!!
\maketitle
\abstract{Tutorial for time series analysis in R... }
\tableofcontents
\pagebreak
\section{Introduction}%------------------------------------------------------------------------------------
This tutorial assumes that the reader has some basic knowledge of time series analysis, and the principal focus of the tutorial is not to explain time series analysis, but rather to explain how to carry out these analyses using R.
\noindent 
If you are new to time series analysis, and want to learn more about any of the concepts presented here, We would highly recommend the Open University book “Time series” (product code M249/02), available from from the Open University Shop.

%\begin{itemize}
%\item Definition time series\\
%\item examples in economy, nature, humans,.... \\
%\item stochastic/deterministic with dormann revision\\
%\item stationary / non stationary \\
%\item regression: why time series regression instead of linear standrard regression\\
%\item where you need to use time series regression.\\
%\end{itemize}


\section{Getting started}%------------------------------------------------------------------------------------
\subsection{Packages}
Before we get started, please make sure to set a working directory and download the necessary packages listed below.\\

\noindent Useful packages for time series analysis:

\begin{Schunk}
\begin{Sinput}
> library(tseries)
> library(nlme)
> library(car)
> library(knitr)
> library(xtable)
> library(SweaveListingUtils)
> library(stats)
> library(forecast)
> library(AICcmodavg)
> library(TTR)
> library(mgcv)
\end{Sinput}
\end{Schunk}
\subsection{Functions needed lateron}
function writing to organize our script:\\

\noindent first we can write a diagnostics function with all the tests we need to perform to check if our model is adequate enough to stop the model adaptation.\\
we need to be careful if we want to check for residuals or the whole model. 
so x will be the model and x$residuals and x$fitted are the other options we need.\\
\begin{Schunk}
\begin{Sinput}
> diagnostics <- function (x)
+   {
+   normality = shapiro.test(x$residuals); #check for normal distributed values #   
+   stat.res =  adf.test(x$residuals); #check both residuals and fitted of the model for stationarity
+   stat.fit = adf.test(x$fitted);
+   x$residualsvector = as.vector(x$residuals);
+  autocorr= dwt(x$residualsvector) ; #check for autocorrelation
+  indep=  Box.test(x$residuals, type="Ljung-Box") #check for independence
+   #lag for season is df: m-1 ( 12-1)
+   #write if seasonal = TRUE lag=12-1, else write nothing 
+   #there is high evidence that there are non-zero autocorr. 
+   output = list(normality, stat.res, stat.fit, autocorr, indep)
+   names (output) = c("norm. distrb. of residuals", "stationarity of residuals", "stationarity of fitted values", "autocorrelation of residuals", "independence of residuals")
+   return ( output )
+ }
\end{Sinput}
\end{Schunk}


\noindent Lateron for the forecast plotting, we need the histogram with the normal distribution to see wether the errors of the forecast model are well distributed: 

\begin{Schunk}
\begin{Sinput}
> plotForecastErrors <- function(forecasterrors)
+ {
+   # make a histogram of the forecast errors:
+   mybinsize <- IQR(forecasterrors)/4
+   mysd   <- sd(forecasterrors)
+   mymin  <- min(forecasterrors) - mysd*5
+   mymax  <- max(forecasterrors) + mysd*3
+   # generate normally distributed data with mean 0 and standard deviation mysd
+   mynorm <- rnorm(10000, mean=0, sd=mysd)
+   mymin2 <- min(mynorm)
+   mymax2 <- max(mynorm)
+   if (mymin2 < mymin) { mymin <- mymin2 }
+   if (mymax2 > mymax) { mymax <- mymax2 }
+   # make a red histogram of the forecast errors, with the normally distributed data overlaid:
+   mybins <- seq(mymin, mymax, mybinsize)
+   hist(forecasterrors, col="red", freq=FALSE, breaks=mybins)
+   # freq=FALSE ensures density
+   # generate normally distributed data with mean 0 and standard deviation mysd
+   myhist <- hist(mynorm, plot=FALSE, breaks=mybins)
+   # plot the normal curve as a blue line on top of the histogram of forecast errors:
+   points(myhist$mids, myhist$density, type="l", col="blue", lwd=2)
+ }
\end{Sinput}
\end{Schunk}



\subsection{Dataset (CO2-Concentrations)}%------------------------------------------------------------------------------------
The first dataset we will work with consists of monthly CO2-concentrations [ppm] in the atmosphere, measured over time at the famous Mauna Loa Station on Hawaii.\\
To download this dataset, just use the code provided below.

\begin{Schunk}
\begin{Sinput}
> url<-"ftp://aftp.cmdl.noaa.gov/products/trends/co2/co2_mm_mlo.txt"
> dest<-"C:/Users/schnuri/Desktop/Neuer Ordner/Dataset/run.txt"
> download.file(url, dest ) 
> co2month=read.table(dest, skip=72)
> co2month
\end{Sinput}
\end{Schunk}

\noindent Note:"dest'' represents a randomly chosen  name for a text file in which the CO2-dataset will be saved. Feel free to adjust the name and directory.


\subsection{Dataset Visualization}%-----------------------------------------------------------------------------------
It can be really useful to visualize your dataset before you transform it into a timeseries (ts) in order to detect potential errors.

\subsubsection{Histogram \& QQ-Plot}
\begin{Schunk}
\begin{Sinput}
> data = co2month[,c(3,5)]
> colnames(data)= c("year", "co2")
> attach(data)
> x = co2
> op = par(mfrow = c(1,2),
+           mar = c(5,4,1,2)+.1,
+           oma = c(0,0,2,0))
> hist(co2, freq=F, col = "cornsilk",xlab = "", main = "")
> qqnorm(x, main = ""); qqline(x,col = 'red')
> par(op)
> mtext("CO2 Concentration (ppm) Histogram and QQ Plot", line = 2.5,font = 2,cex = 1.0)
\end{Sinput}
\end{Schunk}
\includegraphics{sweaveclean-fig1check}


\subsubsection{Plotting the fitted values}
\begin{figure}[H]
\centering
\begin{Schunk}
\begin{Sinput}
> # Run a linear model
> datalm = lm( co2 ~ year)
> # Fit predict values
> MyData=data.frame(year=seq(from=(1958),to=2014, by=0.1))
> pred=predict(datalm, newdata=MyData, type="response", se=T)
> # Plot the fitted values
> plot(year, co2, type="n",las=1, xlab="Year", ylab="CO2 conc. (ppm)", main="CO2 concentration in the atmosphere")
> grid (NULL,NULL, lty = 6, col = "cornsilk2") 
> points(year, co2, col="cornflowerblue" )
> # Write confidence interval 
> F=(pred$fit)
> FSUP=(pred$fit+1.96*pred$se.fit) # make upper conf. int. 
> FSLOW=(pred$fit-1.96*pred$se.fit) # make lower conf. int. 
> lines(MyData$year, F, lty=1, col="red", lwd=3)
> lines(MyData$year, FSUP,lty=1, col="red", lwd=3)
> lines(MyData$year, FSLOW,lty=1, col="red", lwd=3)
> legend("topleft",c("simple linear regression y~x", "monthly mean data"), 
+        pch=c(20,20), col=c("red", "cornflowerblue"))
\end{Sinput}
\end{Schunk}
\includegraphics{sweaveclean-fig1datalm}
\end{figure}
Look at standard errors, highly underestimated, standard errors are higher in true ! #p value is not right evidence , misleading :

\begin{Schunk}
\begin{Sinput}
> xtable(summary(datalm))
\end{Sinput}
% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Tue Nov 25 10:38:01 2014
\begin{table}[ht]
\centering
\begin{tabular}{rrrrr}
  \hline
 & Estimate & Std. Error & t value & Pr($>$$|$t$|$) \\ 
  \hline
(Intercept) & -2618.5566 & 16.9782 & -154.23 & 0.0000 \\ 
  year & 1.4944 & 0.0085 & 174.86 & 0.0000 \\ 
   \hline
\end{tabular}
\end{table}\end{Schunk}


\noindent This plot (~\ref{fig1datalm} can be used to observe if there are outliers which could possibly bias the model. \\
However the poly-1 linear regression is not accurate in fitting the CO2-dataset. This is due to the present autocorrelation that not yet has been taken into account. Neglecting this factor will always effect the accuracy of the model results. The standard errors are lower than  their true values thus giving high statistical significance with a p-value lower than it should be. The clue in statistical modelling is to present the correct statistical evidence, which would be highly biased with a linear model.\\


\subsection{Dataset Transformation}
It is essential to transform your dataset into a timeseries (ts) if you seek for an accurate and extensive analysis of the data.

\noindent The data stored as a dataframe needs to be transformed with the important columns into the class of a time series to continue working on it properly. If you have monthly data you have to set the deltat of the function ts() to deltat=1/12 describing the sampling period parts between successive values xt and xt+1. Your time series should somehow look like table 1.\\

\noindent \textbf{Original Data}\\
\begin{Schunk}
\begin{Sinput}
> xtable(head(data), caption="Original CO2-Data")
\end{Sinput}
% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Tue Nov 25 10:38:01 2014
\begin{table}[ht]
\centering
\begin{tabular}{rrr}
  \hline
 & year & co2 \\ 
  \hline
1 & 1958.21 & 315.71 \\ 
  2 & 1958.29 & 317.45 \\ 
  3 & 1958.38 & 317.50 \\ 
  4 & 1958.46 & 317.10 \\ 
  5 & 1958.54 & 315.86 \\ 
  6 & 1958.62 & 314.93 \\ 
   \hline
\end{tabular}
\caption{Original CO2-Data} 
\end{table}\end{Schunk}
\noindent \textbf{Transformation}\\
\begin{Schunk}
\begin{Sinput}
> yourts=ts(co2, c(1958,3),c(2014,10), deltat=1/12)
> class(yourts)
\end{Sinput}
[1] "ts"\end{Schunk}
%<<results=tex, echo=FALSE, print=TRUE >>=
%print(xtable(yourts, digits=2, label="yourts",caption="Your time series for monthly mean data"), size="\\tiny")
%@

%\pagebreak

\subsection{Time-Series Visualization}%------------------------------------------------------------------------------------
It is important to get a quick overview of your data. Some simple plots for visualization are quite helpful.
\subsubsection{Time-Series Plot}
\begin{figure}[H]
\centering
\begin{Schunk}
\begin{Sinput}
> par(mfrow=c(1,1))
> plot.ts(yourts,las=1, xlab="Year", ylab="CO2 conc. (ppm)", main="CO2 concentration in the atmosphere")
> grid (NULL,NULL, lty = 6, col = "cornsilk2")
> points(yourts, col="cornflowerblue" )
> k <- 5
> lines(year,filter(co2, rep(1/k,k) ),col = 'red', type="l", lwd = 3)
> legend("topleft",c("simple moving average", "monthly mean data"),
+ pch=c(20,20), col=c("red", "cornflowerblue"))
\end{Sinput}
\end{Schunk}
\includegraphics{sweaveclean-fig1visualize}
\caption{Visualization of the CO2 Concentrations}
\label{fig1visualize}
\end{figure}
\noindent \textbf{Note: The red line in plot \ref{fig1visualize} was computed with a simple moving average. It is not enough to just run a MA.}\\

%\pagebreak

\subsubsection{ACF, PACF, SPECTRUM}
\noindent Since time-series data usually violates the independence assumption of the model, the standard error is potentially too small. As the data are regularly spaced in time, we can easily employ the autocorrelation function to investigate residuals correlations in the model errors.\\
\textbf{The acf() function} can be used for that, which produces a plot of the correlogram.\\
Another nice procedure is to run the \textbf{autocorrelation  function} with its complementary \textbf{partial acf} and the \textbf{spectrum}  showing the spectral density of your time series at frequencies corresponding to the possibly approx. Fourier frequencies. \\ 

\begin{figure}[H]
\centering
\begin{Schunk}
\begin{Sinput}
> op <- par(mfrow = c(3,1),
+           mar = c(2,4,1,2)+.1,
+           oma = c(0,0,2,0))
> acf(x, xlab = "")
> pacf(x, xlab = "")
> spectrum(x, xlab = "", main = "")
> par(op)
> mtext("CO2 Concentration (ppm) correlogram", 
+       line = 2.5, 
+       font = 2, 
+       cex = 0.8)
> op <- par(mfrow = c(3,1),
+           mar = c(2,4,1,2)+.1,
+           oma = c(0,0,2,0))
> acf(resid(datalm), xlab = "")
> pacf(resid(datalm),xlab = "")
> spectrum(resid(datalm), xlab = "", main = "")
> par(op)
> mtext("Model residual correlogram", 
+       line = 2.5, 
+       font = 2, 
+       cex = 1.2)
\end{Sinput}
\end{Schunk}
\includegraphics{sweaveclean-correlogram}
\caption{Correlogram of time series and residuals of lm}
\label{correlogram}
\end{figure}


Explain the acf , pacf, spectrum here. \\
\textbf{ACF \& PACF:}\\
\noindent The generated correlogram  reveals that there are major autocorrelations. \\
There is a strong correlation at lag 1,a weaker correlation at lag 2, and a noticeable correlation at lag 3. Such a correlation pattern is typical for an autoregressive process where most of the sequential dependence can be explained as a  flow-on effect from a dependence at lag 1.\\

\noindent In an autoregressive time series, an independent error component, or ''innovation" is associated with each time point. For an order p autoregressive time series, the error for any time point is obtained by taking the innovation for that time point, and adding a linear combination of the innovations at the p previous time points. (For the present time series, initial indications are that p = 1 might capture most of the correlation structure.)"\\ (autosmooth.pdf)\\

\noindent \textbf{Spectrum:} easier to interpret the acf / log scaled / strong cycles where spectrum max. occurs 
Here the highest maximum is at about 0.75. 1/0.75 = 12 meaning a 12 month cycle is occuring here .\\

\noindent The residuals in a time series are serially correlated. The ACF is waving and decreases only slowly, which could be an identification of non-stationarity ( If the ACF would drop to zero quickly, the time series would be stationary). We stop all diagnostics here for our clearly wrong model and go on to investigate the different components of our time series.\\


\section{Decomposition of Time Series}%-----------------------------------------------------------------------------------------------------
A time serie consists of 3 components; a trend component, an irregular (random) component and (if it is a seosonal time series) seasonal component.

\subsection{Decomposing Seasonal Data}%---------------------------------------------------------------------------------------------------- 

\noindent We can decompose the ts and plot these components:

\begin{figure}[H]
\centering
\begin{Schunk}
\begin{Sinput}
> plot(decompose(yourts)) 
> 
\end{Sinput}
\end{Schunk}
\includegraphics{sweaveclean-decompose}
\caption{Decomposition of the CO2 Time Series}
\end{figure}


\noindent We can see each component with:
\begin{Schunk}
\begin{Sinput}
> yourts_components<- decompose(yourts)
\end{Sinput}
\end{Schunk}
\begin{Schunk}
\begin{Sinput}
> yourts_components$seasonal
\end{Sinput}
\end{Schunk}


\begin{figure}[H]
\centering
\begin{Schunk}
\begin{Sinput}
> #we can see the trend for the first year:
> par(mfrow=c(1,2))
> ts.plot(yourts_components$seasonal[1:12])
> ts.plot(aggregate(yourts_components$seasonal))
> 
> 
\end{Sinput}
\end{Schunk}
\includegraphics{sweaveclean-decomposition}
\caption{The seasonal component across the time}
\label{decomposition}
\end{figure}

It seems that our seasonal component is positive until the summer months, were it turns to be negative and turning to be positiv again in winter (see fig. ~\ref{decomposition}). And we can see in the right plot that this seasonal component is constant over all the years (see fig. ~\ref{decomposition}).

\begin{figure}[H]
\centering
\begin{Schunk}
\begin{Sinput}
> yourts_seasonallyadjusted <- yourts - yourts_components$seasonal
> par(mfrow=c(1,2))
> plot(yourts, main="TS with seasonal fl.", las=1)
> plot(yourts_seasonallyadjusted, las=1, main="removed seasonal fluctuation")
> 
\end{Sinput}
\end{Schunk}
\includegraphics{sweaveclean-seasonallyadjusted}
\caption{Comparison of seasonal vs. seasonally adjusted model}
\label{decomposition2}
\end{figure}


\noindent It seems that our data can probably be described using an additive model, since the random fluctuations in the data are roughly constant in size over time (constant seasonal component). In some cases it might be handy to have the model without the seasonal fluctuations to depict change in the trend and local extremes easier (see fig. ~\ref{decomposition2}).

\section{Analysis of Seasonal Data}


After looking at the simple linear regression datalm, we were facing some serious problems with our model. 
To be sure about non-stationarity of our time series,  wen can run the adf.test, giving us the result that our data is non-stationary and we need to fix it: 

\begin{Schunk}
\begin{Soutput}
	Augmented Dickey-Fuller Test

data:  yourts
Dickey-Fuller = -1.0663, Lag order = 8, p-value = 0.928
alternative hypothesis: stationary
\end{Soutput}
\end{Schunk}

Also we need a model which is covering the serial correlation of our residuals. The ACF, PACF, spectrum above gives us certainty an the autocorrelation and the seasonality. 
The standard errors are highly underestimated, thus in the summary the p-value is too small and misleading. 

A nice model to try is GLS, which will allow for correlation of standard errors and unequel variances. 

In the GLS we have different options to choose, though our data is not spatially correlated we are not discussing spatial autocorrelation here ( further reading on:...)

Our first try on gls will be simple: 
\begin{Schunk}
\begin{Sinput}
> data.glsAR = gls(co2 ~ year,cor= corAR1(acf(resid(datalm))$acf[2])) 
> 
\end{Sinput}
\end{Schunk}

The difference will be made in the correlation structure. There are generally ( for temporal corr. interesting) five options you have: 
\begin{enumerate}
  \item corAR1: in ACF exponential decreasing values of correlation with time distance\\
  \item corARMA: either autoregressive order or moving average order or both\\
  \item corCAR1: continuous time ( time index does nto have to be integers)\\
  \item corCompSymm: correlation does not decrease with higher distance\\
  \item corSymm: general correlation only for few observations only, often overparameterized\\
\end{enumerate}

Our first gls model accounts for the AR1, which is clearly visible in the PACF. 
\begin{Schunk}
\begin{Sinput}
> acf(data.glsAR$residuals)
\end{Sinput}
\end{Schunk}

We have still a lot of problems concerning the autocorrelation and the seasonality. One option is to allow the AR to use more parameters and/or to include a moving average or error variance to the model. This can be handled via the corARMA. We tried 2 versions, one with 1 lag and 1 moving average, the other with 2 lags and 2 moving averages. 
The 0.2 are starting values for Phi, which are in the modelling process optimized. 

The next models are thus: 
\begin{Schunk}
\begin{Sinput}
> data.glsARMA1 = gls ( co2 ~ year, cor = corARMA (c(0.2,0.2),p=1, q=1 )) 
> data.glsARMA2 = gls (co2 ~year, cor=corARMA(c(0.2,0.2,0.2, 0.2), p=2, q=2)) 
> xtable(anova(data.glsAR, data.glsARMA1, data.glsARMA2))
\end{Sinput}
% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Tue Nov 25 10:40:03 2014
\begin{table}[ht]
\centering
\begin{tabular}{rlrrrrrlrr}
  \hline
 & call & Model & df & AIC & BIC & logLik & Test & L.Ratio & p-value \\ 
  \hline
data.glsAR & gls(model = co2 \~{} year, correlation = corAR1(acf(resid(datalm))\$acf[2])) &   1 &   4 & 2189.53 & 2207.61 & -1090.77 &  &  &  \\ 
  data.glsARMA1 & gls(model = co2 \~{} year, correlation = corARMA(c(0.2, 0.2), p = 1,     q = 1)) &   2 &   5 & 1760.02 & 1782.62 & -875.01 & 1 vs 2 & 431.51 & 0.00 \\ 
  data.glsARMA2 & gls(model = co2 \~{} year, correlation = corARMA(c(0.2, 0.2, 0.2,     0.2), p = 2, q = 2)) &   3 &   7 & 1494.81 & 1526.44 & -740.40 & 2 vs 3 & 269.21 & 0.00 \\ 
   \hline
\end{tabular}
\end{table}\end{Schunk}
To compare all the models we use anova and the best model is  so far the data.glsARMA2 with the lowest AIC and significantly better than the ARMA1, which is itself significantly better than the data.glsAR. 
$`norm. distrb. of residuals`

	Shapiro-Wilk normality test

data:  x$residuals
W = 0.9785, p-value = 1.861e-08


$`stationarity of residuals`

	Augmented Dickey-Fuller Test

data:  x$residuals
Dickey-Fuller = -1.0665, Lag order = 8, p-value = 0.928
alternative hypothesis: stationary


$`stationarity of fitted values`

	Augmented Dickey-Fuller Test

data:  x$fitted
Dickey-Fuller = -72220583361, Lag order = 8, p-value = 0.01
alternative hypothesis: stationary


$`autocorrelation of residuals`
[1] 0.1111791

$`independence of residuals`

	Box-Ljung test

data:  x$residuals
X-squared = 603.7779, df = 1, p-value < 2.2e-16
\begin{figure}[H]
\begin{Schunk}
\begin{Sinput}
> par(mfrow=c(1,2))
> plot(datalm)
> plot(data.glsARMA2) 
> #well spread is already smaller of variances 
\end{Sinput}
\end{Schunk}
\includegraphics{sweaveclean-residual}
\caption{Residuals fitted vs. observed}
\label{residual}
\end{figure}
#function writing: if anova$AIC lowest, choose this as bestmodel
But still we have seasonal problems here. We should therefore include a seasonal term. 
Try to fit for seasonal component and autocorrelation with best corstruct: 
\begin{Schunk}
\begin{Sinput}
> seas = cycle(yourts)
> dataseason.gls = gls(co2 ~ year + factor(seas), cor=corARMA( c(0.2,0.2,0.2, 0.2),p=2, q=2)) #use bestmodel corstruct